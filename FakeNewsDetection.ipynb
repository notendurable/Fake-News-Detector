{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xlrd'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxlrd\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mgetcwd())\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xlrd'"
     ]
    }
   ],
   "source": [
    "# To add a new cell, type '# %%'\n",
    "# To add a new markdown cell, type '# %% [markdown]'\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import xlrd\n",
    "print(os.getcwd())\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from PIL import Image\n",
    "from wordcloud import ImageColorGenerator\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import re,string,unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# %%\n",
    "True_News = pd.read_csv('TrueNews.csv')\n",
    "True_News.head()\n",
    "True_News.shape\n",
    "\n",
    "\n",
    "# %%\n",
    "Fake_News = pd.read_csv('FakeNews.csv')\n",
    "Fake_News.head()\n",
    "Fake_News.shape\n",
    "\n",
    "\n",
    "# %%\n",
    "True_News['target'] = 1\n",
    "Fake_News['target'] = 0 \n",
    "\n",
    "\n",
    "# %%\n",
    "True_News.head()\n",
    "# True_News.count()\n",
    "True_News.info()\n",
    "\n",
    "\n",
    "# %%\n",
    "Fake_News.head()\n",
    "Fake_News.count()\n",
    "\n",
    "\n",
    "# %%\n",
    "News_data = pd.concat([True_News, Fake_News], ignore_index=True, sort=False)\n",
    "News_data.tail()\n",
    "News_data.count()\n",
    "News_data.info()\n",
    "\n",
    "\n",
    "# %%\n",
    "import string\n",
    "# def punctuation_removal(text):\n",
    "#    all_list = [char for char in text if char not in string.punctuation]\n",
    "#    clean_str = ''.join(all_list)\n",
    "#    return clean_str\n",
    "News_data['text'] = News_data['text'].apply(lambda y: ''.join([char for char in y if char not in string.punctuation]))\n",
    "Fake_News['text'] = Fake_News['text'].apply(lambda y: ''.join([char for char in y if char not in string.punctuation]))\n",
    "True_News['text'] = True_News['text'].apply(lambda y: ''.join([char for char in y if char not in string.punctuation]))\n",
    "\n",
    "\n",
    "# %%\n",
    "News_data.head()\n",
    "\n",
    "\n",
    "# %%\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize\n",
    "useless = stopwords.words('english')\n",
    "News_data['text'] = News_data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (useless) ]))\n",
    "News_data['text'][0]\n",
    "Fake_News['text'] = Fake_News['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (useless) ]))\n",
    "True_News['text'] = True_News['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (useless) ]))\n",
    "Count_vect_test = CountVectorizer()\n",
    "# token_space_test = tokenize.WhitespaceTokenizer()\n",
    "# word_token = tokenize(News_data['text'])\n",
    "\n",
    "#print(News_data.target[0])\n",
    "\n",
    "#  for i, row in News_data.iterrows():\n",
    "#    if News_data.target[i] ==1:\n",
    "#         print(\"ok\")\n",
    "#        # all_words_test_fake = ' '.join([text for text in News_data['text']])    \n",
    "#    else:\n",
    "#        print(\"11111111\")\n",
    "    \n",
    "# all_words_test_fake = ' '.join([text for text in News_data['text']])\n",
    "\n",
    "# token_phrase_test = word_tokenize(all_words_test_fake)\n",
    "\n",
    "\n",
    "# %%\n",
    "# for i, row in News_data.iterrows():\n",
    "#    if News_data.target[i] ==1:\n",
    "#        all_words_test_fake = ' '.join(News_data['text'])\n",
    "#    else:\n",
    "#        all_words_test_real = ' '.join([News_data['text']])    \n",
    "\n",
    "# all_words_test_fake = ' '.join([text for text in News_data['text'] if News_data.target.value==1])\n",
    "\n",
    "# all_words_test_fake=  ' '.join([text for text in News_data['text']])\n",
    "\n",
    "\n",
    "# token_phrase_fake = word_tokenize(all_words_test_fake)\n",
    "# print(token_phrase_test[:100])\n",
    "# token_phrase_real = word_tokenize(all_words_test_real)\n",
    "\n",
    "all_words_test_fake= ' '.join([x for x in Fake_News['text'] ])\n",
    "all_words_test_true = ' '.join([x for x in True_News['text'] ])\n",
    "token_phrase = tokenize.WhitespaceTokenizer()\n",
    "token_phrase_fake = token_phrase.tokenize(all_words_test_fake)\n",
    "token_phrase_true = token_phrase.tokenize(all_words_test_true)\n",
    "\n",
    "print(token_phrase_fake[:100])\n",
    "print(token_phrase_true[:100])\n",
    "# token_phrase_real = word_tokenize(all_words_test_real)\n",
    "\n",
    "\n",
    "# %%\n",
    "frequency_test_fake = nltk.FreqDist(token_phrase_fake)\n",
    "# print(type(frequency_test))\n",
    "df_frequency_test_fake = pd.DataFrame({\"Word\": list(frequency_test_fake.keys()),\n",
    "                                   \"Frequency\": list(frequency_test_fake.values())})\n",
    "df_frequency_test_fake = df_frequency_test_fake.nlargest(columns = \"Frequency\",n= 50)\n",
    "print(df_frequency_test_fake)\n",
    "frequency_test_true = nltk.FreqDist(token_phrase_true)\n",
    "# print(type(frequency_test))\n",
    "df_frequency_test_true = pd.DataFrame({\"Word\": list(frequency_test_true.keys()),\n",
    "                                   \"Frequency\": list(frequency_test_true.values())})\n",
    "df_frequency_test_true = df_frequency_test_true.nlargest(columns = \"Frequency\",n= 50)\n",
    "print(df_frequency_test_true)\n",
    "# NEED TO DIVIDE BET\n",
    "\n",
    "\n",
    "# %%\n",
    "plt.figure(figsize=(12,8))\n",
    "ax = sns.barplot(data = df_frequency_test_fake, x = \"Word\", y = \"Frequency\", color=\"grey\")\n",
    "ax.set(ylabel = \"Count\")\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# %%\n",
    "plt.figure(figsize=(12,8))\n",
    "ax = sns.barplot(data = df_frequency_test_true, x = \"Word\", y = \"Frequency\", color=\"grey\")\n",
    "ax.set(ylabel = \"Count\")\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "print(Fake_News.groupby(['subject'])['text'].count())\n",
    "Fake_News.groupby(['subject'])['text'].count().plot(kind=\"bar\")\n",
    "plt.show()\n",
    "print(True_News.groupby(['subject'])['text'].count())\n",
    "True_News.groupby(['subject'])['text'].count().plot(kind=\"bar\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# %%\n",
    "# token_space_test2 = tokenize.WhitespaceTokenizer()\n",
    "\n",
    "# all_words_test2 = ' '.join([text for text in News_data['text']])\n",
    "# token_phrase_test2 = token_space_test2.tokenize(all_words_test2)\n",
    "# print(len(token_phrase_test2))\n",
    "# print(type(token_phrase_test2))\n",
    "# frequency2 = nltk.FreqDist(token_phrase_test2)\n",
    "# df_frequency_test2 = pd.DataFrame({\"Word\": list(frequency2.keys()),\n",
    "#                                    \"Frequency\": list(frequency2.values())})\n",
    "# df_frequency_test2 = df_frequency_test2.nlargest(columns = \"Frequency\",n= 50)\n",
    "# print(df_frequency_test2)\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "print(News_data.groupby(['subject'])['text'].count())\n",
    "News_data.groupby(['subject'])['text'].count().plot(kind=\"bar\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# %%\n",
    "print(News_data.groupby(['target'])['text'].count())\n",
    "News_data.groupby(['target'])['text'].count().plot(kind='bar')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# %%\n",
    "from wordcloud import WordCloud\n",
    "fake_data = News_data[News_data['target'] == 0]\n",
    "all_words = ' '.join([text for text in fake_data.text])\n",
    "wordcloud = WordCloud(width= 800, height= 500,\n",
    "                          max_font_size = 110,\n",
    "                          collocations = False).generate(all_words)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# %%\n",
    "from wordcloud import WordCloud\n",
    "real_data = News_data[News_data['target'] == 1]\n",
    "all_words = ' '.join([text for text in real_data.text])\n",
    "wordcloud = WordCloud(width= 800, height= 500, max_font_size = 110,\n",
    " collocations = False).generate(all_words)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# %%\n",
    "# Most frequent words counter (Code adapted from https://www.kaggle.com/rodolfoluna/fake-news-detector)   \n",
    "from nltk import tokenize\n",
    "token_space = tokenize.WhitespaceTokenizer()\n",
    "def counter(text, column_text, quantity):\n",
    "    all_words = ' '.join([text for text in text[column_text]])\n",
    "    token_phrase = token_space.tokenize(all_words)\n",
    "    frequency = nltk.FreqDist(token_phrase)\n",
    "    df_frequency = pd.DataFrame({\"Word\": list(frequency.keys()),\n",
    "                                   \"Frequency\": list(frequency.values())})\n",
    "    df_frequency = df_frequency.nlargest(columns = \"Frequency\", n = quantity)\n",
    "    print(df_frequency)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    ax = sns.barplot(data = df_frequency, x = \"Word\", y = \"Frequency\", color = 'blue')\n",
    "    ax.set(ylabel = \"Count\")\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# %%\n",
    "counter(News_data[News_data['target'] == 0], 'text', 50)\n",
    "\n",
    "\n",
    "# %%\n",
    "counter(News_data[News_data['target'] == 1], 'text', 50)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Function to plot the confusion matrix (code from https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],horizontalalignment=\"center\",color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(News_data['text'], News_data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Vectorizing and applying TF-IDF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', LogisticRegression())])\n",
    "# Fitting the model\n",
    "model = pipe.fit(X_train, y_train)\n",
    "# Accuracy\n",
    "prediction = model.predict(X_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\n",
    "\n",
    "\n",
    "# %%\n",
    "cm = metrics.confusion_matrix(y_test, prediction)\n",
    "plot_confusion_matrix(cm, classes=[0, 1])\n",
    "\n",
    "\n",
    "# %%\n",
    "# Vectorizing and applying TF-IDF\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', MultinomialNB())])\n",
    "# Fitting the model\n",
    "model = pipe.fit(X_train, y_train)\n",
    "# Accuracy\n",
    "prediction = model.predict(X_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\n",
    "\n",
    "\n",
    "# %%\n",
    "cm = metrics.confusion_matrix(y_test, prediction)\n",
    "plot_confusion_matrix(cm, classes=[0, 1])\n",
    "\n",
    "\n",
    "# %%\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Vectorizing and applying TF-IDF\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', DecisionTreeClassifier(criterion= 'entropy',\n",
    "                                           max_depth = 20, \n",
    "                                           splitter='best', \n",
    "                                           random_state=42))])\n",
    "# Fitting the model\n",
    "model = pipe.fit(X_train, y_train)\n",
    "# Accuracy\n",
    "prediction = model.predict(X_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\n",
    "\n",
    "\n",
    "# %%\n",
    "cm = metrics.confusion_matrix(y_test, prediction)\n",
    "plot_confusion_matrix(cm, classes=[0,1])\n",
    "\n",
    "\n",
    "# %%\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', RandomForestClassifier(n_estimators=50, criterion=\"entropy\"))])\n",
    "model = pipe.fit(X_train, y_train)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\n",
    "\n",
    "\n",
    "# %%\n",
    "cm = metrics.confusion_matrix(y_test, prediction)\n",
    "plot_confusion_matrix(cm, classes=[0,1])\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
